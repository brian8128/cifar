Here is output from the large gpu ec2 instance.  It is going about 5x faster than locally.  This is similar performance
to what I would expect from the small gpu ec2 instance so I suspect something is going wrong.



ubuntu@ip-172-31-54-154:~$ git clone https://github.com/datamath28/cifar.git
Cloning into 'cifar'...
remote: Counting objects: 22, done.
remote: Compressing objects: 100% (16/16), done.
remote: Total 22 (delta 5), reused 13 (delta 3), pack-reused 0
Unpacking objects: 100% (22/22), done.
Checking connectivity... done.
ubuntu@ip-172-31-54-154:~$ ls
cifar  tensorflow
ubuntu@ip-172-31-54-154:~$ cd cifar/
ubuntu@ip-172-31-54-154:~/cifar$ ls
BUILD             cifar10_input_test.py       cifar10_train.py
cifar10_eval.py   cifar10_multi_gpu_train.py  __init__.py
cifar10_input.py  cifar10.py                  README.md
ubuntu@ip-172-31-54-154:~/cifar$ python cifar10_multi_gpu_train.py --num_gpus=4
>> Downloading cifar-10-binary.tar.gz 100.0%
Succesfully downloaded cifar-10-binary.tar.gz 170052171 bytes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 32
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties:
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 1 with properties:
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:04.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 2 with properties:
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:05.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 3 with properties:
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:06.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 0 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 0 to device ordinal 2
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 0 to device ordinal 3
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 1 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 1 to device ordinal 2
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 1 to device ordinal 3
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 2 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 2 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 2 to device ordinal 3
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 3 to device ordinal 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 3 to device ordinal 1
I tensorflow/core/common_runtime/gpu/gpu_init.cc:45] cannot enable peer access from device ordinal 3 to device ordinal 2
I tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0 1 2 3
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y N N N
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 1:   N Y N N
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 2:   N N Y N
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 3:   N N N Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:644] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:644] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GRID K520, pci bus id: 0000:00:04.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:644] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GRID K520, pci bus id: 0000:00:05.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:644] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GRID K520, pci bus id: 0000:00:06.0)
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 3927400448
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 3927400448
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 3927400448
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 3927392256
I tensorflow/core/common_runtime/direct_session.cc:45] Direct session inter op parallelism threads: 32
2016-01-15 21:46:08.862269: step 0, loss = 4.68 (7.2 examples/sec; 17.793 sec/batch)
2016-01-15 21:46:15.368488: step 10, loss = 4.66 (894.6 examples/sec; 0.143 sec/batch)
W tensorflow/core/common_runtime/gpu/pool_allocator.cc:227] PoolAllocator: After 617683 get requests, put_count=617450 evicted_count=1000 eviction_rate=0.00161956 and unsatisfied allocation rate=0.00215806
2016-01-15 21:46:21.547327: step 20, loss = 4.64 (945.4 examples/sec; 0.135 sec/batch)
2016-01-15 21:46:27.600676: step 30, loss = 4.62 (969.5 examples/sec; 0.132 sec/batch)
2016-01-15 21:46:33.558226: step 40, loss = 4.60 (876.1 examples/sec; 0.146 sec/batch)
2016-01-15 21:46:39.598549: step 50, loss = 4.58 (843.6 examples/sec; 0.152 sec/batch)
2016-01-15 21:46:46.076554: step 60, loss = 4.57 (811.8 examples/sec; 0.158 sec/batch)
2016-01-15 21:46:52.820745: step 70, loss = 4.54 (768.2 examples/sec; 0.167 sec/batch)
2016-01-15 21:46:58.860681: step 80, loss = 4.53 (786.6 examples/sec; 0.163 sec/batch)
2016-01-15 21:47:05.689972: step 90, loss = 4.51 (848.2 examples/sec; 0.151 sec/batch)
2016-01-15 21:47:12.070112: step 100, loss = 4.49 (850.7 examples/sec; 0.150 sec/batch)
2016-01-15 21:47:18.968882: step 110, loss = 4.48 (939.8 examples/sec; 0.136 sec/batch)
2016-01-15 21:47:25.093316: step 120, loss = 4.46 (737.1 examples/sec; 0.174 sec/batch)
2016-01-15 21:47:31.421792: step 130, loss = 4.44 (809.2 examples/sec; 0.158 sec/batch)
2016-01-15 21:47:37.973334: step 140, loss = 4.42 (747.0 examples/sec; 0.171 sec/batch)
2016-01-15 21:47:44.259564: step 150, loss = 4.41 (728.8 examples/sec; 0.176 sec/batch)
2016-01-15 21:47:50.513760: step 160, loss = 4.39 (772.3 examples/sec; 0.166 sec/batch)
2016-01-15 21:47:56.941541: step 170, loss = 4.38 (966.2 examples/sec; 0.132 sec/batch)
2016-01-15 21:48:03.528856: step 180, loss = 4.36 (817.7 examples/sec; 0.157 sec/batch)
2016-01-15 21:48:09.958625: step 190, loss = 4.34 (743.3 examples/sec; 0.172 sec/batch)
2016-01-15 21:48:16.396173: step 200, loss = 4.32 (832.4 examples/sec; 0.154 sec/batch)

Yup, low gpu utilization.

ubuntu@ip-172-31-54-154:~$ nvidia-smi
Fri Jan 15 21:55:57 2016
+------------------------------------------------------+
| NVIDIA-SMI 346.46     Driver Version: 346.46         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |
| N/A   46C    P0    50W / 125W |   3833MiB /  4095MiB |     27%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GRID K520           Off  | 0000:00:04.0     Off |                  N/A |
| N/A   41C    P0    54W / 125W |   3833MiB /  4095MiB |     16%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GRID K520           Off  | 0000:00:05.0     Off |                  N/A |
| N/A   47C    P0    60W / 125W |   3833MiB /  4095MiB |     20%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GRID K520           Off  | 0000:00:06.0     Off |                  N/A |
| N/A   43C    P0    51W / 125W |   3833MiB /  4095MiB |     13%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1881    C   python                                        3820MiB |
|    1      1881    C   python                                        3820MiB |
|    2      1881    C   python                                        3820MiB |
|    3      1881    C   python                                        3820MiB |
+-----------------------------------------------------------------------------+


To try:
See if we can get a better version of cuda.  This will require more disk?  It works fine if you use a different filesystem.  However the latesst version of cuda is already installed.   Maybe the graphics cards themselves are too old to have direct peer to peer access.  

http://tleyden.github.io/blog/2014/10/25/cuda-6-dot-5-on-aws-gpu-instance-running-ubuntu-14-dot-04/

Saving to: ‘cuda_6.5.14_linux_64.run’

59% [===============================================>                                  ] 574,466,440 12.9MB/s   in 55s


Cannot write to ‘cuda_6.5.14_linux_64.run’ (No space left on device).


SINGLE GPU INSTANCE:

* Actually faster than the multi gpu instance, at least at the beginning.  

ubuntu@ip-172-31-35-170:~/cifar$ python cifar10_multi_gpu_train.py
>> Downloading cifar-10-binary.tar.gz 100.0%
Succesfully downloaded cifar-10-binary.tar.gz 170052171 bytes.
Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
I tensorflow/core/common_runtime/local_device.cc:25] Local device intra op parallelism threads: 8
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:888] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:88] Found device 0 with properties:
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:112] DMA: 0
I tensorflow/core/common_runtime/gpu/gpu_init.cc:122] 0:   Y
I tensorflow/core/common_runtime/gpu/gpu_device.cc:644] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
I tensorflow/core/common_runtime/gpu/gpu_region_allocator.cc:47] Setting region size to 3927400448
I tensorflow/core/common_runtime/direct_session.cc:45] Direct session inter op parallelism threads: 8
2016-01-17 04:58:26.700677: step 0, loss = 4.67 (2.7 examples/sec; 48.089 sec/batch)
2016-01-17 04:58:31.225004: step 10, loss = 4.66 (341.8 examples/sec; 0.374 sec/batch)
2016-01-17 04:58:35.012559: step 20, loss = 4.63 (338.0 examples/sec; 0.379 sec/batch)
2016-01-17 04:58:38.809236: step 30, loss = 4.62 (333.6 examples/sec; 0.384 sec/batch)
2016-01-17 04:58:42.617642: step 40, loss = 4.60 (336.6 examples/sec; 0.380 sec/batch)
2016-01-17 04:58:46.441223: step 50, loss = 4.58 (342.2 examples/sec; 0.374 sec/batch)
2016-01-17 04:58:50.259566: step 60, loss = 4.57 (331.0 examples/sec; 0.387 sec/batch)
2016-01-17 04:58:54.082335: step 70, loss = 4.54 (341.2 examples/sec; 0.375 sec/batch)
2016-01-17 04:58:57.911303: step 80, loss = 4.53 (333.4 examples/sec; 0.384 sec/batch)
2016-01-17 04:59:01.752244: step 90, loss = 4.52 (334.4 examples/sec; 0.383 sec/batch)
2016-01-17 04:59:05.556159: step 100, loss = 4.50 (336.9 examples/sec; 0.380 sec/batch)
2016-01-17 04:59:09.820938: step 110, loss = 4.47 (335.0 examples/sec; 0.382 sec/batch)
2016-01-17 04:59:13.637574: step 120, loss = 4.46 (336.2 examples/sec; 0.381 sec/batch)
2016-01-17 04:59:17.469681: step 130, loss = 4.44 (328.4 examples/sec; 0.390 sec/batch)
2016-01-17 04:59:21.294973: step 140, loss = 4.42 (332.0 examples/sec; 0.386 sec/batch)
2016-01-17 04:59:25.116577: step 150, loss = 4.41 (339.9 examples/sec; 0.377 sec/batch)
2016-01-17 04:59:28.954683: step 160, loss = 4.38 (328.6 examples/sec; 0.390 sec/batch)
2016-01-17 04:59:32.792696: step 170, loss = 4.37 (331.1 examples/sec; 0.387 sec/batch)
2016-01-17 04:59:36.596193: step 180, loss = 4.37 (334.0 examples/sec; 0.383 sec/batch)
2016-01-17 04:59:40.398684: step 190, loss = 4.34 (327.9 examples/sec; 0.390 sec/batch)
2016-01-17 04:59:44.251013: step 200, loss = 4.32 (334.7 examples/sec; 0.382 sec/batch)

Still getting lower GPU usage than I'd expect.  This may be due to the fact that I'm in multi gpu mode.
ubuntu@ip-172-31-35-170:~$ nvidia-smi
Sun Jan 17 05:04:59 2016
+------------------------------------------------------+
| NVIDIA-SMI 346.46     Driver Version: 346.46         |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GRID K520           Off  | 0000:00:03.0     Off |                  N/A |
| N/A   48C    P0    60W / 125W |   3833MiB /  4095MiB |     42%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0      1501    C   python                                        3820MiB |
+-----------------------------------------------------------------------------+

Single GPU mode failed at first but now succeedes after I ran the multi gpu mode.  Performance is similar, perhaps slightly faster.  
